{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIrxUHm9sHho"
   },
   "source": [
    "**IMPORT SECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTG8d3-ruIMb",
    "outputId": "a260e3a2-c088-486f-9f2a-cbe23fab1329"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\acer\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "!pip install beautifulsoup4\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "Cj9fXBNwZVtd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import reimport nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy\n",
    "import math\n",
    "import wikipedia\n",
    "import requests\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import MDS\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import scipy\n",
    "import math\n",
    "import requests\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import MDS\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QevS3MtSsSj0"
   },
   "source": [
    "**GET TEXT FROM WIKIPEDIA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "9BWym90Ma0x9"
   },
   "outputs": [],
   "source": [
    "titles=wikipedia.search('spiderman')\n",
    "result=wikipedia.page(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len of docs\",len(result.content))\n",
    "print(\"type of docs\",type(result.content))\n",
    "print(\"content\",result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charbased_preprocessing(content):\n",
    "    tokens = []\n",
    "    for char in content:\n",
    "        tokens.append(char)\n",
    "    return(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=np.array(charbased_preprocessing(result.content)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "chars = encoder.fit_transform(chars).toarray()\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DIVIDE TEXT TO SEQUENCES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "sequence_length = 10\n",
    "for i in range(0,chars.shape[0]-sequence_length):\n",
    "    x.append(chars[i:i+sequence_length])\n",
    "    y.append(chars[i+sequence_length])\n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INDEX TO CHAR DICT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = set([c for c in result.content])\n",
    "dic=sorted(dic)\n",
    "index2char = dict((i, c) for i, c in enumerate(dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9O9gCL8tlc9"
   },
   "source": [
    "**TRAINING PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the size of the input and output layers\n",
    "input_size = chars.shape[1] \n",
    "output_size = chars.shape[1]\n",
    "# Define the hyperparameters\n",
    "hidden_size = 100  # Size of the hidden state\n",
    "sequence_length = 10  # Length of the input sequence\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the RNN model with multiple layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, input_shape=(sequence_length, input_size)),\n",
    "    tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(hidden_size),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x, y, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICT NEXT CHAR TO GENERATE TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random sequence from x\n",
    "random_index = np.random.randint(0, len(x))\n",
    "random_sequence = x[random_index]\n",
    "\n",
    "# Reshape the random sequence to fit the model input shape\n",
    "random_sequence = np.reshape(random_sequence, (1, sequence_length, input_size))\n",
    "\n",
    "predicted_char_prob = model.predict(random_sequence)\n",
    "predicted_char_index = np.argmax(predicted_char_prob)\n",
    "\n",
    "# Convert the predicted character index to the actual character using index2char dictionary\n",
    "predicted_char = index2char[predicted_char_index]\n",
    "\n",
    "# Convert the random sequence back to characters using index2char dictionary\n",
    "sequence_chars = [index2char[np.argmax(char)] for char in random_sequence[0]]\n",
    "\n",
    "# Print the random sequence and the predicted character\n",
    "print(\"Random Sequence:\", ''.join(sequence_chars))\n",
    "print(\"Predicted Next Character:\", predicted_char)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHAR-BASED GENERATOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_index = np.random.randint(0, len(chars) - sequence_length)\n",
    "sequence_chars = [index2char[np.argmax(char)] for char in chars[sequence_index:sequence_index + sequence_length]]\n",
    "\n",
    "generated_text = ''.join(sequence_chars) \n",
    "random_index = np.random.randint(0, len(x))\n",
    "random_sequence = x[random_index]\n",
    "\n",
    "for _ in range(100):\n",
    "    current_sequence = np.reshape(random_sequence, (1, sequence_length, input_size))\n",
    "    predicted_char_prob = model.predict(current_sequence)\n",
    "    predicted_char_index = np.argmax(predicted_char_prob)\n",
    "    predicted_char = index2char[predicted_char_index]\n",
    "    generated_text += predicted_char\n",
    "    encoded_char=np.zeros(random_sequence.shape[1])\n",
    "    encoded_char[predicted_char_index]=1\n",
    "    random_sequence =np.vstack((random_sequence[1:],encoded_char))\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
